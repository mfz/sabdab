{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6847cb71",
   "metadata": {},
   "source": [
    "## Parse CD-HIT cluster file\n",
    "\n",
    "In notebook 05 we ran cd-hit on the VH.fa and VL.fa files which assigned every sequence to a cluster.\n",
    "\n",
    "Now we want to remove antibodies that are redundant. We consider antibodies to be redundant if they show high sequence similarity of both their heavy and light chains, i.e. if their heavy chains are in the same cluster and their light chains are in the same cluster, respectively.\n",
    "\n",
    "To do so, we create a DataFrame with columns\n",
    "- pbs_id\n",
    "- Hchain\n",
    "- Lchain\n",
    "- Hcluster\n",
    "- Lcluster\n",
    "\n",
    "where the first 3 columns are parsed from the summary file, and the last 2 columns are populated by parsing the cluster files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f7f21",
   "metadata": {},
   "source": [
    "### Parsing the cluster file    \n",
    "\n",
    "\n",
    "The cluster file looks like"
   ]
  },
  {
   "cell_type": "raw",
   "id": "647819b4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ">Cluster 0\n",
    "0\t141aa, >6urh... *\n",
    ">Cluster 1\n",
    "0\t138aa, >6ml8... *\n",
    "1\t137aa, >5umn... at 97.81%\n",
    "2\t137aa, >4fp8... at 100.00%\n",
    ">Cluster 2\n",
    "0\t137aa, >3k3q... *\n",
    ">Cluster 3\n",
    "0\t136aa, >5o0w... *\n",
    ">Cluster 4\n",
    "0\t136aa, >6oca... *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cfb9d3",
   "metadata": {},
   "source": [
    "Lines starting with '>' start a new cluster.\n",
    "\n",
    "The other lines contain the sequence id of the sequences belonging to that cluster enclosed by '>' on the left and three dots '...' on the right.\n",
    "\n",
    "\n",
    "How can we extract the pdb_id from those lines?\n",
    "\n",
    "Given a line, we can use the `find` method to obtain the indices of those enclosing characters and use the string between those indices as pdb_id. An example is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe98d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5o0w\n"
     ]
    }
   ],
   "source": [
    "line = \"0\t136aa, >5o0w... *\"\n",
    "\n",
    "# we want to extract 5o0w \n",
    "# the first character starts after the >\n",
    "# the last character ends before the first '...' \n",
    "\n",
    "# find the first > on the line\n",
    "startidx = line.find('>') \n",
    "# find the first three points \n",
    "endidx = line.find('...')\n",
    "\n",
    "# be aware of python indexing. Slice a:b includes a but not b\n",
    "print(line[(startidx+1):endidx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99feec",
   "metadata": {},
   "source": [
    "Write a function `parse_pdb_id(line)` that returns the pdb_id.\n",
    "\n",
    "We also need to make sure that there is a pdb_id found on the line.\n",
    "The `find` method returns -1 if the substring is not found.\n",
    "\n",
    "Assume a pdb_id is found when startidx != -1 and endidx != -1 and endidx - startidx > 3.\n",
    "If no pdb_id is found, `raise ValueError(f\"No pbs_id found in {line}\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b65a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdb_id(line):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3fd89a",
   "metadata": {},
   "source": [
    "Test the function on a few examples to see it works as expected. You want to test both successful execution and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd9282",
   "metadata": {},
   "source": [
    "Now we can parse the cluster file.\n",
    "\n",
    "Write a function `parse_cluster_file(cluster_file)` that\n",
    "\n",
    "- declares empty dictionary pdb2cluster\n",
    "- sets current_cluster to ''\n",
    "- opens cluster_file (use a `with` block)\n",
    "- loops over all lines\n",
    "  - if line starts with a '>'\n",
    "    - reassign current_cluster\n",
    "  - else\n",
    "    - parse pdb_id\n",
    "    - set pdb2cluster[pdb_id] = current_cluster\n",
    "- return pdb2cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cluster_file(cluster_file):\n",
    "    ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ae62d",
   "metadata": {},
   "source": [
    "### Annotating summary file with cluster numbers\n",
    "\n",
    "- parse VH cluster file into pdb2vhcluster\n",
    "- parse VL cluster file into pdb2vlcluster\n",
    "\n",
    "- load the summary DataFrame (summary_pdb.tsv)\n",
    "- create two additional columns Hcluster and Vcluster and populate them with the clusters\n",
    " \n",
    "  Here you loop over pdb_ids (you can use a list comprehension) and find the cluster from the corresponding dictionary.\n",
    "  As some pdb_ids could not be processed, not all pdb_ids will be in the dictionary.\n",
    "  Use `pdb2vhcluster.get(pdb_id, '')` to look up the key, and return '' if it does not exist.\n",
    "  \n",
    "\n",
    "- sort the DataFrame (see below)\n",
    "- create a column duplicated that indicates if Hcluster and Vcluster entries of a row are duplicated (pandas has a `.duplicated(subset=[...])` method. Use it on subset of Hcluster and Vcluster columns.)\n",
    "\n",
    "- save the DataFrame as `../generated/preprocess/summary_pdb_clusters.tsv`\n",
    "\n",
    "- drop duplicated lines (select lines where duplicated is False)\n",
    "\n",
    "- save the DataFrame without duplicates as `../generated/preprocess/summary_pdb_clusters_dedup.tsv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5dc94",
   "metadata": {},
   "source": [
    "There is on caveat here. If we have duplicated anitbodies, we want to keep those instances that are the best. By best here we mean they have affinity data and good resolution. We can achieve this when we understand how the duplicated method works. If we use `df.duplicated(subset=[..], keep = 'first')`, the function marks all duplicates as `True` except for the first one. \n",
    "\n",
    "So all we need to do is sort the DataFrame such that columns with affinity value appear first and columns with good resolution as well. And then apply the `duplicated` method on the sorted DataFrame. Check the documentation of `sort_values`, and pay attention to `na_position` and `ascending` options. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
